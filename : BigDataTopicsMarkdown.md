# Big Data Topics Markdown
 
## <ins> Big Data with example and types</ins>
Data which is too large or complex to process or manipulate in a given time frame.

<ins>Example</ins> : Weather prediciton data from diffrent sources from diffrent complex systems and sensors that send volumes of data perodicly. <br> 
## <ins> 6 ‘V’s of Big Data (define each)
1. Volume :<br>
	In big data, volumes mean that there would be lots and lots  of volumes of data that are very hard to store or manipulate. The volume of the data can come from any different sources. It is very challenging because the data collection continues to increase and makes it very complex to store. When we say volumes we are dealing with petabytes of data from one or different data sources 
2. Variety :<br>
	The second v refers to the variety of data that is being stored in the system and it requires even more processing. Example in social media platforms there is lots of data we can experience such as the structured, unstructured, semi structured, photos, videos, meta data, location data, medical, GPS, instrumental data that are to be processed. 
3. Velocity : <br>
	Velocity is the amount of data that has been received for an instance of a particular time is high. Storing these high velocities of data is very complex because of the input and output capacity. The storage is very limited in terms of the bandwidth. Also velocity of the data often makes the system crash because some of the systems are provisioned to limited IPOS(input output per second) and does not scale based on the velocity of the data.   
4. Value : <br> 
	The Value of big data refers to the value of the data that has been vetted or received that can provide valuable insights that can be used to derive a meaningful conclusion from the data itself. Without the value that the big data provides is very useful, other data is useless that doest provide anything that is not useful to them storing.
5. Veracity :<br>
	Veracity refers to the reliability of the data that we are getting from the sources. The truth and authenticity of the data that has been received is otherwise called the veracity of the data.
6. Variability : <br>
	Variability of the data refers to how fast the structure of the data. If the structure of the data keeps changing it is very hard for the system to change based on the data because of the change in the data and its corresponding structure, the model also changes dynamically and results in a different output. 

## <ins> Phases of Big Data analysis : (discuss each)
1. Phase 1: Data acquisition and Recording<br> 
	It refers to the data that has been collected from the various sources over the different mediums and storing the data that is collected for the future usage of the data to form meaning from the data. 
2. Phase 2: Information Extraction and Cleaning<br>
	If the data has meaning or has value then that data is collected and then it is used to extract information from it that is required. The data cleaning is a process in which that removes the noise, values missing out or other outliers that result in the inconsistency in the data is called data cleaning and it is very essential to improve the accuracy of the data. 
3. Phase 3: Data Integration, Aggregation, and Representation<br>
	The data integration and aggregation refers to combining the data from the multiple data sources that can help in making the data insights better. This phase helps in the reduction of the inconsistencies and avoids redundancies. The representation of the data refers to the visualization of the data that we have for easy understanding such as the Bar chart, histogram, Boxplot, Pie chart.
4. Phase 4: Query Processing, Data Modeling and Analysis<br>
	Querying generally refers to deriving a useful set of information from the whole set of data to get specific answers in the overall data. Data modeling is a finding the relationship between the data and its models. Analysis cannot only be made in the SQL querying and some of the big data analysis are not stored in the sql databases and also cannot be implemented in the SQL querying. 
5. Phase 5: Interpretation<br> 
	The Interpretation phase is the final phase that helps us understand the data form the big data and also verify the results that are extracted by processing the data. Data visualization will help in the better understanding of the data.
	
## <ins> Challenges in Big Data analysis (discuss each)
1. Challenge 1: Heterogeneity and Incompleteness:<br>
	Heterogeneity of the data is huge problem in the big data where the system accepts only homogeneous data and if there is any small differences the system cannot understand the contrast and that can be problem even so the data is been cleaned the incompleteness would still remain and cannot be avoided and that arise problematic for the system.	
2. Challenge 2: Scale:<br>
	Scaling can be a problem if there are static compute resources or on premise servers that are very limited and hence the resources that are attached to it like storage and processors are also limited and if the volume scales faster than the expected scale that might be a potential trouble.
3. Challenge 3: Timeliness:<br>
	The timeliness of big data is usually slow and can take several minutes to several hours of processing the data even if it can be a significantly small change but the volume makes it harder to do. Often querying would take a lot of time to fetch desired results.
4. Challenge 4: Privacy:<br>The data privacy is a major concern that is increasing day by day on the internet. To handle a lot of data that corresponds to a lot of people who have their own privacy policy and privacy concerns that associate with data not being available or over sharing of data to be processed. 
5. Challenge 5: Human Collaboration:<br>
	The big data requires human interventions for analysis of the big data even so it requires lot of computational power to process data. Most of the time in big data analysis the system most of the time accepts multiple human experts that can review the results that the model provides. 


